# -*- coding: utf-8 -*-
"""Copy of UCI_Wine_TF_Logistic_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FdTiMx5ksfZ5z36xE66zfJJSop6j69O7
"""

import csv
import urllib.request

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

import tensorflow as tf

# URL of the UCI Wine dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"

# Open the URL and read the data
response = urllib.request.urlopen(url)
csvreader = csv.reader(response.read().decode('utf-8').splitlines())

data = [list(map(float, row)) for row in csvreader]

cols = ['class_label', 'alcohol', 'malic_acid', 'ash',
          'alcalinity_of_ash', 'magnesium', 'total_pphenols',
          'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins',
          'color_intensity', 'hue', 'OD280/OD315_of_diluted_wines',
          'proline']

response = urllib.request.urlopen(url)
csvreader = csv.reader(response.read().decode('utf-8').splitlines())
data2 = [[float(entry) for entry in row] for row in csvreader]
print(data2 == data)

from collections import Counter

Counter([x[0] for x in data])

y = [x[0]-1 for x in data]
#y_bin = [0 if x ==1 else 1 for x in data]
X = [x[1:] for x in data]

import matplotlib.pyplot as plt
plt.scatter([x[2] for x in X],[x[3] for x in X])
plt.show()
#plt.hist([x[2] for x in X])
#plt.show()

#binary
# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

plt.scatter([x[2] for x in X_scaled],[x[3] for x in X_scaled])
plt.show()

y = [x[0]-1 for x in data]
y_bin = [0 if x==0 else 1 for x in data]
X = [x[1:] for x in data]

#binary
# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_bin, test_size=0.3)

# Convert to TensorFlow tensors
X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)
y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)
X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)
y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)

# Logistic regression model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))
    ])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_tf, y_train_tf, epochs=200, batch_size=16, verbose=0)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_tf, y_test_tf, verbose=0)
accuracy

#scaler = StandardScaler()
##X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
#scaler.fit(X_train)
#X_scaled =scaler.transform(X_train)
#X_test = scaler.transform(X_test)

from tensorflow.keras.utils import to_categorical

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)


# One-hot encode the labels for multi-class classification
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# Multi-class logistic regression model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=y_train_encoded.shape[1], activation='softmax', input_shape=(X_train.shape[1],))
    ])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_encoded, epochs=200, batch_size=16, verbose=0)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)
print(accuracy)

model.predict(X_test[2:3])[0]

y_test_encoded[2]

# now with NN
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(units=y_train_encoded.shape[1], activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_encoded, epochs=100, batch_size=16, verbose=0)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)
print(accuracy)

# Remember

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier().fit(X_train, y_train)
clf.score(X_test,y_test)

clf.predict_proba(X_test[0:1])

plt.hist(clf.feature_importances_)

import keras
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.preprocessing import sequence

# Load the dataset
top_words = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

# Pad the sequences
max_words = 500
X_train2 = sequence.pad_sequences(X_train, maxlen=max_words)
X_test2 = sequence.pad_sequences(X_test, maxlen=max_words)

# Convert the sequences to a bag-of-words model
import numpy as np

def vectorize_sequences(sequences, dimension=top_words):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1  # set specific indices of results[i] to 1s
    return results

X_train3 = vectorize_sequences(X_train2, dimension=top_words)
X_test3 = vectorize_sequences(X_test2, dimension=top_words)

# Build the model
model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(top_words,)))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print the model summary
model.summary()

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)

# Evaluate the model
scores = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

X_train[0]
np.where(X_train2[0])

